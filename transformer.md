## attention is all your need 

Q1: softmax函数中$\sqrt{d}$的作用？


Q2: 多头注意力机制的用处和实现？


Q3: position embedding的作用是什么？


Q4: embedding layer 乘以$\sqrt{d}$的作用？


Q5: 相比于RNN，Transformer为什么处理时序数据更快？

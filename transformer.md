## attention is all your need 

### Q1: softmax函数中$\sqrt{d}$的作用？

[softmax 求导](https://www.zhihu.com/tardis/zm/art/105758059?source_id=1003)

### Q2: 多头注意力机制的用处和实现？


### Q3: position embedding的作用是什么？


### Q4: embedding layer 乘以$\sqrt{d}$的作用？


### Q5: 相比于RNN，Transformer为什么处理时序数据更快？



### Q6: Decoder 中 masked attention 作用？
